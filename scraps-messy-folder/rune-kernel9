   {
      dim3 block((n*hfrac), 1, 1);
      dim3 grid (m, 1, 1);

      unsigned long int elapsed;
      struct timeval t_start, t_end, t_diff;
      gettimeofday(&t_start, NULL);

      // GPU call to kernel 9
      ker9 <<< grid, block, (n*hfrac)*sizeof(float) >>> (hfrac, n, m, N, d_hs, d_yerrs, d_nss, d_MOfsts);
      cudaDeviceSynchronize();

      gettimeofday(&t_end, NULL);
      timeval_subtract(&t_diff, &t_end, &t_start);
      elapsed = (t_diff.tv_sec*1e6+t_diff.tv_usec);

      // check for cuda errors
      gpuAssert( cudaPeekAtLastError() );

      // copy result from device to host
      cudaMemcpy(h_MOfsts, d_MOfsts, MO_size, cudaMemcpyDeviceToHost);

      // validation 
      printEf(fpV, h_MOfsts, m);

      printf("GPU Optimized Kernel 9 runs in: %lu microsecs\n", elapsed);
      // float microsecPerMatrixMul = elapsed;
      // double flopsPerMatrixMul = 2.0 * HEIGHT_A * WIDTH_B * WIDTH_A;
      // double gigaFlops = (flopsPerMatrixMul * 1.0e-9f) / (microsecPerMatrixMul / (1000.0f * 1000.0f));
      // printf( "GPU Optimized Kernel 9 Performance= %.2f GFlop/s, Time= %.3f microsec %d %d\n", gigaFlops, microsecPerMatrixMul, grid.x, grid.y);
   }



   __global__ void ker9(float hfrac, uint n, uint m, uint N, int* hs, float* yerrs, uint* nss, float* MOfsts) {
    extern __shared__ volatile float shmem1[];
    volatile float* sh_yerrs = (volatile float*)shmem1;

    uint  pix = blockIdx.x;    // grid.x: m 
    uint  i   = threadIdx.x;   // blockDim.x: n * hfrac
    int   h   = hs[pix]; 
    float tmp = 0.0;

    if (i < h && pix < m) {
        uint ns = nss[pix];
        float yerr = yerrs[pix*N + i + ns-h+1];
        tmp = yerr;
    }

    sh_yerrs[threadIdx.x] = tmp;

    __syncthreads();

    tmp = scanIncBlock<Add<float> >(sh_yerrs, threadIdx.x);
    
    if (threadIdx.x == blockDim.x-1) {
        MOfsts[pix] = tmp;
    }
}